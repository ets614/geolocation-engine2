# Prometheus Alerting Rules for Detection API
# SLO-driven alerts with error budget tracking

apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: detection-api-alerts
  namespace: monitoring
  labels:
    prometheus: detection-api
    role: alert-rules
spec:
  groups:
    # --------------------------------------------------------------------------
    # SLO: Availability (target: 99.95%)
    # --------------------------------------------------------------------------
    - name: detection-api.availability
      interval: 30s
      rules:
        # Error rate over 5 minutes
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{app="detection-api", status=~"5.."}[5m]))
              /
              sum(rate(http_requests_total{app="detection-api"}[5m]))
            ) > 0.005
          for: 2m
          labels:
            severity: critical
            team: platform
            slo: availability
          annotations:
            summary: "Detection API error rate above 0.5% (SLO: 99.95%)"
            description: "Error rate is {{ $value | humanizePercentage }} over the last 5 minutes."
            runbook_url: "https://runbooks.internal/detection-api/high-error-rate"

        # Error budget burn rate (fast burn: 14.4x in 1h)
        - alert: ErrorBudgetFastBurn
          expr: |
            (
              sum(rate(http_requests_total{app="detection-api", status=~"5.."}[1h]))
              /
              sum(rate(http_requests_total{app="detection-api"}[1h]))
            ) > 0.0072
          for: 5m
          labels:
            severity: critical
            team: platform
            slo: availability
          annotations:
            summary: "Error budget burning 14.4x faster than sustainable"
            description: "At this rate, the monthly error budget will be exhausted in {{ printf \"%.1f\" (div 1 (mul $value 2000)) }} hours."

        # Error budget burn rate (slow burn: 3x in 6h)
        - alert: ErrorBudgetSlowBurn
          expr: |
            (
              sum(rate(http_requests_total{app="detection-api", status=~"5.."}[6h]))
              /
              sum(rate(http_requests_total{app="detection-api"}[6h]))
            ) > 0.0015
          for: 30m
          labels:
            severity: warning
            team: platform
            slo: availability
          annotations:
            summary: "Error budget burning 3x faster than sustainable"
            description: "Slow error budget burn detected. Review recent changes."

    # --------------------------------------------------------------------------
    # SLO: Latency (target: p99 < 500ms)
    # --------------------------------------------------------------------------
    - name: detection-api.latency
      interval: 30s
      rules:
        - alert: HighLatencyP99
          expr: |
            histogram_quantile(0.99,
              sum(rate(http_request_duration_seconds_bucket{app="detection-api"}[5m])) by (le)
            ) > 0.5
          for: 5m
          labels:
            severity: warning
            team: platform
            slo: latency
          annotations:
            summary: "Detection API p99 latency above 500ms"
            description: "p99 latency is {{ $value | humanizeDuration }} over the last 5 minutes."

        - alert: HighLatencyP95
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{app="detection-api"}[5m])) by (le)
            ) > 0.3
          for: 5m
          labels:
            severity: warning
            team: platform
            slo: latency
          annotations:
            summary: "Detection API p95 latency above 300ms"
            description: "p95 latency is {{ $value | humanizeDuration }}."

    # --------------------------------------------------------------------------
    # Infrastructure Health
    # --------------------------------------------------------------------------
    - name: detection-api.infrastructure
      interval: 30s
      rules:
        - alert: PodCrashLooping
          expr: |
            increase(kube_pod_container_status_restarts_total{container="detection-api"}[1h]) > 3
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Detection API pod is crash-looping"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour."

        - alert: PodNotReady
          expr: |
            kube_pod_status_ready{condition="true", pod=~".*detection-api.*"} == 0
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Detection API pod not ready"
            description: "Pod {{ $labels.pod }} has been not-ready for 5 minutes."

        - alert: HighCPUUsage
          expr: |
            (
              sum(rate(container_cpu_usage_seconds_total{container="detection-api"}[5m])) by (pod)
              /
              sum(kube_pod_container_resource_limits{container="detection-api", resource="cpu"}) by (pod)
            ) > 0.85
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Detection API CPU usage above 85%"
            description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}."

        - alert: HighMemoryUsage
          expr: |
            (
              sum(container_memory_working_set_bytes{container="detection-api"}) by (pod)
              /
              sum(kube_pod_container_resource_limits{container="detection-api", resource="memory"}) by (pod)
            ) > 0.85
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Detection API memory usage above 85%"
            description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}."

        - alert: PVCSpaceLow
          expr: |
            (
              kubelet_volume_stats_available_bytes{persistentvolumeclaim=~".*detection.*"}
              /
              kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*detection.*"}
            ) < 0.2
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "PVC storage below 20%"
            description: "PVC {{ $labels.persistentvolumeclaim }} has only {{ $value | humanizePercentage }} space remaining."

    # --------------------------------------------------------------------------
    # Business Logic Alerts
    # --------------------------------------------------------------------------
    - name: detection-api.business
      interval: 60s
      rules:
        - alert: OfflineQueueGrowing
          expr: |
            detection_api_offline_queue_size > 1000
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Offline queue has more than 1000 items"
            description: "Queue size is {{ $value }}. TAK server may be unreachable."

        - alert: TAKServerUnreachable
          expr: |
            detection_api_tak_server_healthy == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "TAK server is unreachable"
            description: "TAK server has been unreachable for 5 minutes. Detections are being queued."

        - alert: GeolocationAccuracyDegraded
          expr: |
            avg(detection_api_geolocation_accuracy_meters) > 100
          for: 15m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "Average geolocation accuracy degraded above 100m"
            description: "Average accuracy is {{ $value }}m. Check sensor inputs."

        - alert: NoDetectionsProcessed
          expr: |
            rate(detection_api_detections_processed_total[15m]) == 0
          for: 30m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "No detections processed in 30 minutes"
            description: "The detection pipeline appears to be idle. Verify upstream sources."
